{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.18.0\n",
      "keras: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "\n",
    "import keras\n",
    "print('keras: %s' % keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "\tmodel = VGG16()\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\tprint(model.summary())\n",
    "\tfeatures = dict()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\timage = load_img(filename, target_size=(224, 224))\n",
    "\t\timage = img_to_array(image)\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\timage_id = name.split('.')[0]\n",
    "\t\tfeatures[image_id] = feature\n",
    "\t\tprint('>%s' % name)\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "print(Image.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:  # Open the file with UTF-8 encoding\n",
    "        text = file.read()  # Read the file content\n",
    "    return text  # Return the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document loaded successfully!\n",
      "First 200 characters: 1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA lit\n"
     ]
    }
   ],
   "source": [
    "filename = r'C:\\Users\\geets\\Desktop\\code\\new_ml\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "doc = load_doc(filename)\n",
    "\n",
    "print(\"âœ… Document loaded successfully!\")\n",
    "print(\"First 200 characters:\", doc[:200])  # Verify contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document loaded successfully!\n",
      "Type of doc: <class 'str'>\n",
      "First 200 characters: 1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA lit\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:  # Ensure the file opens properly\n",
    "        return file.read()  # Read and return the file content directly\n",
    "\n",
    "# Define the correct file path\n",
    "filename = r'C:\\Users\\geets\\Desktop\\code\\new_ml\\Flickr8k_text\\Flickr8k.token.txt'\n",
    "\n",
    "# Load the document\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# Check if it's loaded correctly\n",
    "print(\"âœ… Document loaded successfully!\")\n",
    "print(\"Type of doc:\", type(doc))\n",
    "print(\"First 200 characters:\", doc[:200])  # Print a sample to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\t\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\t\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = list()\n",
    "\t\t\n",
    "\t\tmapping[image_id].append(image_desc)\n",
    "\treturn mapping\n",
    "\n",
    "\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Descriptions loaded: 8092\n"
     ]
    }
   ],
   "source": [
    "descriptions = load_descriptions(doc)\n",
    "print('âœ… Descriptions loaded:', len(descriptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    " \n",
    "def clean_descriptions(descriptions):\n",
    "\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\t\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\t\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\n",
    "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\t\n",
    "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
    " \n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def to_vocabulary(descriptions):\n",
    "\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    " \n",
    "\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_descriptions(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\tlines.append(key + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    " \n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "doc = load_doc(filename)\n",
    "\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "\n",
    "clean_descriptions(descriptions)\n",
    "\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "\t\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t\n",
    "\t\ttokens = line.split()\n",
    "\t\t\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "\t\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in directory: 8091\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = r'C:\\Users\\geets\\Desktop\\code\\new_ml\\Flickr8k_Dataset\\Flicker8k_Dataset'\n",
    "images = [f for f in os.listdir(directory) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
    "print(f\"Number of images in directory: {len(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "descriptions_file = r\"C:\\Users\\geets\\Desktop\\code\\new_ml\\Flickr8k_text\\descriptions.txt\"\n",
    "\n",
    "print(\"File exists?\", os.path.exists(descriptions_file))  # Should print True if the file is present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File loaded successfully!\n",
      "ðŸ“œ First 5 lines:\n",
      "1000268201_693b08cb0e child in pink dress is climbing up set of stairs in an entry way\n",
      "1000268201_693b08cb0e girl going into wooden building\n",
      "1000268201_693b08cb0e little girl climbing into wooden playhouse\n",
      "1000268201_693b08cb0e little girl climbing the stairs to her playhouse\n",
      "1000268201_693b08cb0e little girl in pink dress going into wooden cabin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "descriptions_file = r\"C:\\Users\\geets\\Desktop\\code\\new_ml\\descriptions.txt\"\n",
    "\n",
    "# Test if the file can be read\n",
    "try:\n",
    "    with open(descriptions_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.readlines()\n",
    "\n",
    "    print(\"âœ… File loaded successfully!\")\n",
    "    print(f\"ðŸ“œ First 5 lines:\\n{''.join(content[:5])}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: File not found!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ERROR: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total images for training: 8092\n",
      "ðŸ” Sample image IDs: ['523249012_a0a25f487e', '1295698260_e10c53c137', '3547368652_0d85c665d3', '2955099064_1815b00825', '3673970325_4e025069e9']\n"
     ]
    }
   ],
   "source": [
    "descriptions_file = r\"C:\\Users\\geets\\Desktop\\code\\new_ml\\descriptions.txt\"\n",
    "\n",
    "# Extract all unique image IDs from descriptions.txt\n",
    "train_dataset = set()\n",
    "\n",
    "with open(descriptions_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) > 1:  # Ensure there's an ID and description\n",
    "            image_id = tokens[0]  # First token is the image ID\n",
    "            train_dataset.add(image_id)\n",
    "\n",
    "print(f\"âœ… Total images for training: {len(train_dataset)}\")\n",
    "print(\"ðŸ” Sample image IDs:\", list(train_dataset)[:5])  # Show first 5 IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load text from a file\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total descriptions loaded: 8092\n"
     ]
    }
   ],
   "source": [
    "train_descriptions = load_clean_descriptions(descriptions_file, train_dataset)\n",
    "print(\"âœ… Total descriptions loaded:\", len(train_descriptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        all_desc.extend(descriptions[key])  # Flattening all descriptions into a single list\n",
    "    return all_desc\n",
    "\n",
    "# Fit a tokenizer on the given caption descriptions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"âœ… Tokenizer is working! Vocabulary Size:\", vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint to save the best model during training\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.keras'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# If you are training the model, use this checkpoint in the `callbacks` list:\n",
    "# model.fit(..., callbacks=[checkpoint], ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.keras'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.layers import Add  # Fix for `add`\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6000 training images.\n",
      "Loaded 6000 training descriptions.\n",
      "Loaded 6000 training features.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.68 GiB for an array with shape (306404, 4096) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Ensure vocab size includes padding\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Now call the function with the correct max_length\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m X1train, X2train, ytrain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_descriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(tokenizer, max_length, descriptions, photos, vocab_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m             X2\u001b[38;5;241m.\u001b[39mappend(in_seq)\n\u001b[0;32m     24\u001b[0m             y\u001b[38;5;241m.\u001b[39mappend(out_seq)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(X2), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.68 GiB for an array with shape (306404, 4096) and data type float32"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pickle import load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a proper max_length (adjust based on your dataset)\n",
    "MAX_LENGTH = 34  # Typical for Flickr8k\n",
    "\n",
    "# Load text file into memory\n",
    "def load_doc(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Load set of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = {line.split('.')[0] for line in doc.split(\"\\n\") if len(line) > 0}\n",
    "    return dataset\n",
    "\n",
    "# Load cleaned descriptions into a dictionary\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = {}\n",
    "\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions.setdefault(image_id, []).append(desc)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "# Load photo features from pickle\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset if k in all_features}\n",
    "    return features\n",
    "\n",
    "# Load training dataset\n",
    "train_images_path = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(train_images_path)\n",
    "print(f\"Loaded {len(train)} training images.\")\n",
    "\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print(f\"Loaded {len(train_descriptions)} training descriptions.\")\n",
    "\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print(f\"Loaded {len(train_features)} training features.\")\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([desc for desc_list in train_descriptions.values() for desc in desc_list])\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Ensure vocab size includes padding\n",
    "\n",
    "# Now call the function with the correct max_length\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, MAX_LENGTH, train_descriptions, train_features, vocab_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
